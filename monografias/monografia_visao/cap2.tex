%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Extração de Características
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Horus}
\label{cap:horus}
 Horus é um \textit{toolkit}, criado no presente trabalho, para desenvolvimento e controle de agentes inteligentes desenvolvido na linguagem de programação Python. Esse \textit{toolkit} foi construído com o objetivo de fornecer classes e algoritmos voltados a resolução de problemas de mapeamento automático de ambientes e visão computacional. O Horus é um projeto \textit{Open Source} que se encontra sob a licença de uso LGPL(\textit{Lesser General Public License}). O projeto está disponível para uso no repositório de projetos \textit{Open Source} da Google na url \textit{http://code.google.com/p/finishstrike/}.

	O \textit{toolkit} Horus fornece os módulos \textit{Core}, Mapeamento, Visão e Util. O módulo \textit{Core} apresenta as abstrações que devem ser implementadas pelas aplicações para construir um agente inteligente. O módulo Mapeamento fornece algoritmos de localização, mapeamento e navegação para um agente. O módulo Visão fornece os algoritmos de visão computacional necessários na etapa de reconhecimento de padrões. Por último, o módulo Util fornece um conjunto de funções utilitárias que podem ser usadas tanto no \textit{toolkit} Horus quanto em qualquer outra aplicação. Cada um desses módulos será explicado nas subseções seguintes.

\section{\textit{Core}}
	O Horus pode ser utilizado tanto como uma biblioteca de algoritmos para processamento de imagens, visão computacional e mapeamento de ambientes, como através de extensões das classes fornecidas pelo módulo \textit{core}. Essas classes podem ser estendidas pelas aplicações, são chamadas de abstrações. A Figura \ref{fig:arquiteturaCore} mostra a arquitetura deste módulo.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.35, bb=0 0 1158 486]{imagens/core.PNG}
\end{center}
\caption{Arquitetura do módulo \textit{core}.}
\label{fig:arquiteturaCore}
\end{figure}

	A arquitetura acima apresenta classes que representam o ambiente (\textit{Environment}), o agente inteligente (\textit{Agent}), os dispositivos (\textit{Device}), o programa de agente (\textit{Brain}), eventos (\textit{Event}) e a hierarquia de comportamentos (\textit{Behavior}). Cada instância da classe \textit{Agent} representa um agente inteligente na aplicação. Para que um agente inteligente possa ser utilizado por uma aplicação, ela deve configurá-lo com instâncias de \textit{Device} e uma única instância da classe \textit{Brain}, responsável pela inteligência do agente.

	O programa de agente, responsável pela inteligência do agente, deve ser implementado em extensões da classe \textit{Brain} (cérebro). Uma aplicação que deseja implementar um programa de agente para um agente em particular deve estender a classe \textit{Brain} e implementar o método \textit{run()} dessa classe, como apresentado na Figura \ref{fig:arquiteturaBrain}. Esse método é o \textit{loop} principal da execução do agente. No diagrama acima, nota-se que a classe \textit{Brain} pode estar relacionada a nenhum ou a muitos comportamentos. Essa é mais uma facilidade fornecida pelo módulo \textit{core} do Horus que tem o objetivo de organizar a implementação dos comportamentos separadamente da implementação da classe \textit{Brain}. Dessa forma, o programa de agente fica mais claro e simples de ser compreendido e mantido.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.4, bb=0 0 561 544]{imagens/brainExtension.PNG}
\end{center}
\caption{Extensão da classe \textit{Brain} do Horus por uma aplicação.}
\label{fig:arquiteturaBrain}
\end{figure}

Os comportamentos (\textit{Behavior}) recebem eventos gerados pela aplicação e fazem com que o agente execute uma determinada ação com base no tipo de evento recebido. Como exemplo de eventos, pode-se citar a captura de uma cena, um obstáculo detectado, a leitura de um determinado dispositivo, etc. Um comportamento pode ser implementado diretamente como uma extensão da classe \textit{Behavior} ou como uma extensão de uma ou várias de suas subclasses. As subclasses da classe \textit{Behavior} (\textit{NeuralNetworkBehavior}, \textit{TesseractBehavior} e \textit{MappingBehavior}) fornecem facilidades para a utilização de funcionalidades fornecidas pelo Horus. Logo, \textit{NeuralNetworkBehavior} fornece métodos para a construção e treinamento de redes neurais na implementação de um comportamento. A classe \textit{TesseractBehavior} disponibiliza a funcionalidade de OCR da \textit{engine} Tesseract, presente no Horus. A classe \textit{MappingBehavior} disponibiliza métodos para implementação de comportamentos de mapeamento de ambientes através da técnica SLAM. Dessa forma, uma aplicação que necessite de um comportamento que envolva mapeamento de ambientes e redes neurais, por exemplo, deve criar uma classe que estenda tanto da classe \textit{MappingBehavior} como da classe \textit{NeuralNetworkBehavior}. A Figura \ref{fig:arquiteturaBehavior} mostra como ficaria o esquema desse comportamento, sendo representado pela classe \textit{MyBehavior}.

A classe \textit{Behavior} também possui atributos para armazenar informações sobre estado de execução de um comportamento. Essas informações são os horários de início e término da execução e o status de execução do comportamento. Essas informações são utilizadas para emissão de relatórios de atuação do agente.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.35, bb=0 0 671 879]{imagens/behaviorExtension.PNG}
\end{center}
\caption{Comportamento \textit{MyBehavior} que estende tanto de \textit{NeuralNetworkBehavior} quanto de \textit{MappingBehavior}.}
\label{fig:arquiteturaBehavior}
\end{figure}

Em certas aplicações, um agente inteligente não necessariamente se encontra sozinho no ambiente. Ele pode interagir com outros agentes, como no caso de aplicações que envolvam enxames de agentes. Para que um agente possa obter informações sobre outros agentes que se encontrem no ambiente, ou sobre o próprio ambiente, foi criada a classe \textit{Environment}, que representa o ambiente no qual o agente está inserido. Dessa forma, o cérebro do agente deve ser configurado tanto com uma instância da classe \textit{Agent}, como com uma instância da classe \textit{Environment} para operar.

Quando o cérebro do agente é configurado com diversos comportamentos, é necessário definir a ordem de execução desses comportamentos. Em alguns casos, os comportamentos podem possuir condições de execução. Logo, o cérebro é responsável por identificar as condições que cada comportamento necessita para ser executado e colocá-lo como ativo quando a sua condição de execução for satisfeita. Contudo, há casos em que dois ou mais comportamentos podem ter a sua condição de execução satisfeita. Nesses casos, é necessário definir prioridades de execução sobre os comportamentos ou executá-los em paralelo. A ordem de execução dos comportamentos define a máquina de estados de execução do agente inteligente. Sendo assim, a implementação do cérebro como uma máquina de estados se enquadra perfeitamente em aplicações que exijam a interação entre diversos comportamentos.

\section{Processamento de Imagem}
\label{image:processing}

O termo processamento de imagens refere-se ao processamento de imagens de duas dimensões por um computador digital \cite{Oge99}, normalmente é utilizado como um estágio para novos processamentos de dados, tais como reconhecimento de padrões e aprendizagem de máquina. Esse tipo de processamento é utilizado em diversos tipos de aplicações, entre elas, processamento de imagens médicas e de satélite, robótica, sensoriamento remoto, entre outras.

Para uma melhor compreensão dos conceitos e algoritmos utilizados durante esse trabalho, é necessária uma breve introdução sobre algumas propriedades de uma imagem digital. Essas propriedades são:

\begin{itemize}

\item Conectividade: esse conceito determina se dois pixels estão conectados entre si. Para isso, é necessário determinar se esses pixels são adjacentes, segundo algum critério, e se os seus níveis de cinza são, de alguma forma, similares. Definindo uma image binária onde os pixels somente assumem valores 0 e 1, dois pixels vizinhos só serão considerados conectados se possuírem o mesmo valor.

\item Adjacência: dois pixels $p$ e $q$ são adjacentes somente se estiverem conectados segundo algum critério. Dados os conjuntos de pixels $C_1$ e $C_2$, esses conjuntos serão adjacentes se algum pixel de $C_1$ é adjacente a algum pixel de $C_2$.

\item Vizinhança: dado um pixel $p$ de coordenadas $(x,y)$, sua 4-vizinhança é definida como $(x+1, y), (x-1, y), (x, y+1), (x, y-1)$, chamada de $N_4(p)$. Os quatro vizinhos diagonais do pixel $p$ são definidos como $(x-1, y-1), (x-1, y+1), (x+1, y-1), (x+1, y+1)$, chamados de $N_d(p)$. Dessa forma, a união dos conjuntos $N_4(p)$ e $N_d(p)$ forma o conjunto da 8-vizinhança do pixel $p$, chamado de $N_8(p)$. A Figura \ref{fig:vizinhanca} ilustra as possíveis vizinhanças de um pixel.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.3, bb=0 0 900 300]{imagens/vizinhanca.png}
\end{center}
\caption{(a) 4-vizinhança (b) d-vizinhança  (c) 8-vizinhança. }
\label{fig:vizinhanca}
\end{figure}



\end{itemize}

Nas próximas subseções serão explicados os principais algoritmos de processamento de imagens implementados no \textit{toolkit} Horus.
\subsection{\textit{Thresholding}}
	O processo de limiarização da imagem, também chamado de \textit{thresholding}, consiste em transformar a imagem que inicialmente se encontra em escala de cinza, com 256 tons diferentes, para uma imagem com apenas dois tons: preto e branco, representados por 0 ou 255 (Figura \ref{fig:imgBin}). No processo de OCR, Essa etapa é utilizada para definir claramente as fronteiras dos caracteres na imagem, a fim de prepará-los para serem analisados pela etapa de extração de características.
	Para transformar uma imagem monocromática de 256 tons de cinza em binária podem ser utilizadas técnicas de \textit{thresholding} global e adaptativo.
	
\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 748 233]{imagens/imagemBinaria.PNG}
\end{center}
\caption{Exemplo de imagem binária.}
\label{fig:imgBin}
\end{figure}

\subsubsection{\textit{Thresholding} Global}
\textit{Thresholding} global é um algoritmo simples que depende de um único parâmetro denominado limiar. O limiar é um valor de intensidade de pixel utilizado como base de comparação. Sendo assim, em uma imagem monocromática de 256 tons de cinza, todo valor de pixel da imagem de entrada que se encontra abaixo do limiar é colocado como 0 e todo valor acima do limiar é colocado como 255 na imagem resultante.
	Tendo $v$ como o valor do pixel na imagem original e $t$ como limiar, o novo valor de pixel computado é:
	 \[ f(v) = \left\{
                                 \begin{array}{ll}
                                     255,  $ v $ \geq $t$\\
                                     0,  $ v $ \leq $t$
                                 \end{array}
                           \right. \]
	 Na Figura \ref{fig:codigo} é apresentado um exemplo de utilização da função \textit{globalThreshold} que pertecem ao módulo de processamento de imagens do Horus.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.7, bb=0 0 531 83]{imagens/codigo.PNG}
\end{center}
\caption{Utilização do \textit{threshold} global implementado no \textit{toolkit} Horus.}
\label{fig:codigo}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.4, bb=0 0 600 315]{imagens/globalThreshold.PNG}
\end{center}
\caption{(a) Imagem original   (b) Resultado do processamento}
\label{fig:output}
\end{figure}

A Figura \ref{fig:output} exibe o resultado do processamento do algoritmo \textit{threshold} global apresentado na Figura \ref{fig:codigo}.  

 Uma da formas de se obter o valor do limiar \textit{t} é através de uma inspeção visual do histograma da imagem. Uma vez que o mesmo limiar é utilizado para toda a imagem, o \textit{thresholding} global pode falhar algumas vezes. No caso de imagens com diferenças de iluminação ou parcialmente sombreadas, informações que não pertencem ao plano de fundo podem ser apagadas. A Figura \ref{fig:thGlobalShadowed} mostra um exemplo de aplicação de \textit{thresholding} global com imagens sombreadas.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.25, bb=0 0 812 509]{imagens/thGlobalShadowed.PNG}
\end{center}
\caption{Imagem original (à direita) e imagem binarizada com thresholding global (à esquerda).}
\label{fig:thGlobalShadowed}
\end{figure}

	Em casos onde a imagem se encontra parcialmente sombreada, a melhor opção é utilizar a técnica de \textit{thresholding} adaptativo.


\subsubsection{\textit{Thresholding} Adaptativo}
No caso de imagens parcialmente sombreadas ou não uniformemente iluminadas, o algoritmo de \textit{thresholding} global falha devido à utilização de um único limiar para a imagem inteira. Isso pode causar o efeito apresentado na Figura \ref{fig:thGlobalShadowed}. O algoritmo de \textit{thresholding} adaptativo resolve esse problema calculando o limiar para cada pixel da imagem com base na sua vizinhança. Há duas abordagens principais de \textit{thresholding} adaptativo: Chow e Kaneko \cite{Chow1972} e \textit{thresholding} local \cite{Gonzales1992}.

\begin{itemize}
\item Abordagem Chow e Kaneko:
 esse método, assim como o de \textit{thresholding} local, se baseia na teoria de que regiões menores da imagem têm maior probabilidade de possuírem uma iluminação aproximadamente uniforme. Com isso, o método de Chow e Kaneko divide a imagem em um vetor de sub-imagens sobrepostas e então encontra o limiar ótimo para cada sub-imagem através da análise de seus histogramas. O limiar para cada pixel é encontrado interpolando os resultados extraídos das sub-imagens. Apesar de gerar resultados satisfatórios, o método de Chow e Kaneko possui um alto custo computacional. Na Figura \ref{fig:chKnk} é mostrado como o limiar de uma sub-imagem é localizado e a posterior interpolação dos valores para encontrar o limiar de cada pixel na sub-imagem.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.4, bb=0 0 707 577]{imagens/chowAndKaneko.PNG}
\end{center}
\caption{Localização do limiar por Chow e Kaneko.}
\label{fig:chKnk}
\end{figure}

\item \textit{Thresholding} Local:
 a segunda forma de encontrar o limiar de cada pixel é uma inspeção estatística da sua vizinhança, levando em consideração que os vizinhos de um pixel tendem a possuir maior chance de ter uma iluminação mais uniforme. No método de \textit{thresholding} local, o limiar de um pixel é definido com base nos valores de um número predeterminado de vizinhos do mesmo. Dessa forma, aplica-se uma operação estatística nos valores da vizinhança de um pixel a fim de obter o seu limiar $t$. Essas operações podem ser a média, a mediana ou a média dos valores mínimo e máximo da vizinhança. Na Figura \ref{fig:imgThLocal} é apresentado o resultado do algoritmo de \textit{threshold} local.


\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.25, bb=0 0 792 510]{imagens/imageThLocal.PNG}
\end{center}
\caption{Imagem original (à direita) e resultado da limiarização com \textit{thresholding} local (à esquerda).}
\label{fig:imgThLocal}
\end{figure}


\end{itemize}

\subsection{Skeletonization}

	\textit{Skeletonization} (Esqueletonização) é o processo de remoção dos pixels de uma imagem, o máximo quanto possível, de forma a preservar a estrutura básica ou esqueleto da imagem. O esqueleto extraído deve ser o mais fino quanto possível (largura de um pixel), conectado e centralizado. Quando estas propriedades são satisfeitas, o algoritmo deve parar. As Figuras \ref{fig:t_sk} e \ref{fig:b_sk} mostram exemplos de imagens e seus respectivos esqueletos.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 310 150]{imagens/t_sk.PNG}
\end{center}
\caption{Imagem de um "T" e seu respectivo esqueleto.}
\label{fig:t_sk}
\end{figure}

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 180 224]{imagens/b_sk.PNG}
\end{center}
\caption{Imagem de um "B" (preto) e seu respectivo esqueleto (branco).}
\label{fig:b_sk}
\end{figure}

	Normalmente, o esqueleto de uma imagem enfatiza as propriedades geométricas e topológicas dos padrões e é extraído quando se deseja preservar as características estruturais da imagem, como por exemplo, junções, \textit{loops} e terminações de linha. Essas características podem ser extraídas do esqueleto para serem utilizadas, posteriormente, em um processo de reconhecimento e classificação de formas através de técnicas de inteligência computacional.

	O algoritmo de \textit{skeletonization} implementado no Horus utiliza o conceito de "\textit{fire front}". Esse conceito realiza a remoção iterativa dos pixels da borda dos padrões até que as condições de conectividade, centralização e espessura do esqueleto sejam satisfeitas. Esse algoritmo, denominado algoritmo de Hilditch, é um processo iterativo em que se aplicam sucessivamente dois passos aos pixels pertencentes à borda de um padrão. O primeiro passo concentra-se em selecionar os pixels das bordas que serão removidos e marcá-los para deleção. O segundo passo é remover todos os pixels marcados para deleção no passo anterior. A Figura \ref{fig:grid} ilustra os oito vizinhos do pixel $p_1$.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=1, bb=0 0 100 100]{imagens/grid.PNG}
\end{center}
\caption{8-vizinhança do pixel $p_1$.}
\label{fig:grid}
\end{figure}

A fim de estabelecer as condições para que um pixel da borda seja marcado para deleção, serão definidas duas funções:
\begin{itemize}
\item $B(p_1)$: número de vizinhos pretos do pixel $p_1$.
\item $A(p_1)$: número de transições de preto para branco $(0$ para $255)$ na seqüência $p_2$, $p_3$, $p_4$, $p_5$, $p_6$, $p_7$, $p_8$, $p_9$.
\end{itemize}

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 300 150]{imagens/grid2_3.PNG}
\end{center}
\caption{Exemplos das funções: (a)$B(p_1)=2$, $A(p_1)=1$  b) $B(p_1)=2$, $A(p_1)=2$.}
\label{fig:grid23}
\end{figure}

A Figura \ref{fig:grid23} mostra exemplos dessas duas funções.


Há duas versões do algoritmo de Hilditch, uma usando uma janela $4\times 4$ e outra usando uma janela $3\times 3$, nesse trabalho foi utilizada uma janela $3\times 3$. Utilizando as funções apresentadas acima, o algoritmo de Hilditch verifica os pixels pretos e marca para deleção aqueles que satisfazem as quatro seguintes condições:

\begin{itemize}
\item $2 \leq B(p_1) \leq 6$: essa condição assegura que o número de vizinhos pretos de um pixel seja maior ou igual a 2 e menor ou igual a 6. Isso garante que nenhuma terminação de linha ou pixel isolado, seja deletada e que o pixel em questão seja um pixel de fronteira.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.2, bb=0 0 1107 420]{imagens/condition1.PNG}
\end{center}
\caption{(a) $B(p_1)=7$ (b) $B(p_1) = 0$ (c) $B(p_1) = 1$.}
\label{fig:condition1}
\end{figure}

	A Figura \ref{fig:condition1} apresenta três condições em que um determinado pixel $p_1$ não deve ser deletado. Quando $B(p_1)$ é igual a $7$, o pixel não é um bom candidato, pois, sua deleção pode quebrar a conectividade do padrão. Quando $B(p_1)$ é igual a $1$, significa que o pixel $p_1$ é uma terminação de linha e já faz parte do esqueleto, portanto, não deve ser removido. Quando $B(p_1)$ é igual a $0$ significa que o pixel $p_1$ é um pixel isolado e também não deve ser removido.

\item $A(p_1) = 1$: essa condição representa efetivamente um teste de conexão. Os casos em que $A(p_1)$ é maior que $1$, a deleção do pixel $p_1$ causa uma quebra na conectividade do padrão, como mostra a Figura \ref{fig:condition2}.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 300 150]{imagens/condition2.PNG}
\end{center}
\caption{Exemplos onde $A(p_1)$ é maior que $1$.}
\label{fig:condition2}
\end{figure}

\item $p_2 + p_3 + p_8 \geq 255 $ ou $ A(p_2) \neq 1$: essa condição assegura que linhas verticais com largura de dois pixels não serão inteiramente removidas pelo algoritmo. A Figura \ref{fig:grid11} apresenta uma situação em que a condição acima é satisfeita.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 100 130]{imagens/grid11.PNG}
\end{center}
\caption{Exemplo de situação em que linhas verticais com largura de dois pixels não serão inteiramente removidas.}
\label{fig:grid11}
\end{figure}

\item $p_2$ + $p_4$ + $p_6 \geq 255$ ou $A(p_4) \neq 1$: essa condição assegura que linhas horizontais com largura de dois pixels não serão inteiramente removidas pelo algoritmo. A Figura \ref{fig:grid13} apresenta uma situação em que a condição acima é satisfeita.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.5, bb=0 0 100 100]{imagens/grid13.PNG}
\end{center}
\caption{Exemplo de situação em que linhas horizontais com largura de dois pixels não serão inteiramente removidas.}
\label{fig:grid13}
\end{figure}
\end{itemize}

	A cada iteração do algoritmo, os pixels das bordas são analisados, alguns deles são marcados para deleção e então deletados. A Figura \ref{fig:sk_demo} ilustra o processo iterativo do algoritmo, onde, os pixels deletados em cada iteração são representados pelas diferenças nos tons de cinza da imagem.

\begin{figure}[!Htb]
\centering
\begin{center}
    \includegraphics[scale=0.3, bb=0 0 555 236]{imagens/sk_demonstration.PNG}
\end{center}
\caption{(a) padrão de entrada do algoritmo (b) deleção iterativa dos pixels das bordas (c) resultado após a execução do algoritmo.}
\label{fig:sk_demo}
\end{figure}

O algoritmo de Hilditch é menos custoso do que o algoritmo de transformação de eixo mediano. Porém, esse algoritmo não funciona perfeitamente para todos os padrões. A Figura \ref{fig:erodedPatterns} apresenta dois tipos de padrões que são completamente erodidos pelo algoritmo.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.4, bb=0 0 337 154]{imagens/erodedPatterns.PNG}
\end{center}
\caption{Padrões completamente erodidos pelo algoritmo de Hilditch.}
\label{fig:erodedPatterns}
\end{figure}

\section{Visão}

	O Sistema de Visão é um dos mais complexos e completos do ser humano, pois fornece um conjunto de informações necessárias à interação do homem com o ambiente. Tal processo se inicia com a captação dos estímulos luminosos do ambiente formando uma imagem, que juntamente aos outros estímulos captados por demais sensores do corpo (som, temperatura, pressão, umidade, cheiro, etc) e as informações contidas na memória, compõem uma cena compreendida pelo cérebro.

	Esse módulo tem como principal objetivo o reconhecimento de padrões.	Na aplicação Ariadnes, desenvolvida neste trabalho, o padrão a ser reconhecido é uma placa com o nome dos locais do ambiente e setas que indicam as direções dos mesmos. Para reconhecimento de uma placa é necessário identificar algumas características de uma imagem, que servirão de padrões de entrada para uma rede neural. 

\subsection{Extração de características}
\label{sec:features}

	Para realizar o reconhecimento de objetos em uma cena, é necessário extrair características das imagens desse objeto, de forma a identificá-lo, independentemente das variações com que ele possa ocorrer na imagem. O 
\textit{toolkit} Horus apresenta alguns algoritmos para extração de características, os principais deles serão explicados nos itens abaixo.
\begin{itemize}
\item Matriz de Pixel: a maneira mais simples de extrair características de um \textit{bitmap} é associar a luminância de cada pixel com um valor numérico correspondente no vetor de características. 

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.35, bb= 0 0 544 308]{imagens/pixelMatrix.PNG}
\end{center}
\caption{Matriz de pixel de um \textit{bitmap}.}
\label{fig:seg}
\end{figure}

   Esse método, apesar de simples, possui alguns problemas que podem torná-lo inadequado para o reconhecimento de caracteres. O tamanho do vetor é igual à altura do \textit{bitmap} multiplicado pela sua largura, portanto, \textit{bitmaps} grandes produzem vetores de características muito longos, o que não é muito adequado para o reconhecimento. Logo, o tamanho do \textit{bitmap} é uma restrição para esse método. Além disso, este método não considera a proximidade geométrica dos pixels, bem como suas relações com a sua vizinhança. No entanto, este método pode ser adequado em situações onde o \textit{bitmap} do caractere se encontra muito opaco ou muito pequeno para a detecção de arestas.
         
            





\item Intensidade de pixel por região: o objetivo deste método de extração de características consiste em extrair a intensidade de pixels pretos em cada região da imagem \cite{Juntanasub2005}. Para isso, divide-se a imagem em $5\times 5$ blocos. Para cada bloco, calcula-se a intensidade de pixels pretos. Isso pode ser calculado pela seguinte equação.

\begin{center}
$\sum_{i=0}^L \sum_{j=0}^M f(x_1, y_1) $
\end{center}

Onde $L$ $M$ representam a altura e largura de cada bloco, respectivamente. 
            
            
\item Histograma de Arestas por Regiões: esse método extrai o número de ocorrências de determinados tipos de arestas em uma região específica do \textit{bitmap}. Isso torna o vetor de características desse método invariante com relação à disposição das arestas em uma região e a pequenas deformações do caractere. Sendo o \textit{bitmap} representado pela função discreta $f (x, y)$, largura $w$ e altura $h$, onde $0 \leq x < w$ e $0 \leq y < h$. Primeiramente é realizada a divisão do \textit{bitmap} em seis regiões $(r_0, r_1, ..., r_5)$ organizadas em três linhas e duas colunas. Outros quatro layouts podem ser utilizados para a divisão do \textit{bitmap} em regiões.

\begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.3, bb= 0 0 369 533]{imagens/layoutsix.PNG}
\end{center}
\caption{Layout com seis regiões em três linhas e duas colunas.}
\label{fig:seg2}
\end{figure}

Definindo a aresta de um caractere como uma matriz $2\times2$ de transições de branco para preto nos valores dos pixels, têm-se quatorze diferentes tipos de arestas, como ilustrado na Figura \ref{fig:seg4}.

\begin{figure}[!htb]
\centering
\begin{center}
   \includegraphics[scale=0.2, bb= 0 0 1128 1042]{imagens/quatorze.PNG}
\end{center}
\caption{Quatorze diferentes tipos de arestas.}
\label{fig:seg4}
\end{figure}



O vetor de ocorrências de cada tipo de aresta em cada sub-região da imagem é normalmente muito longo o que não é uma boa prática em reconhecimento de padrões, onde o vetor de características deve ser tão menor quanto possível. Com isso, pode-se agrupar tipos de arestas semelhantes para reduzir o tamanho do vetor de características. Por questões de simplicidade, o agrupamento dos tipos de aresta será desconsiderado no algoritmo de extração de características. Sendo $n$ igual ao número de tipos de arestas diferentes, onde $h_{i}$ é uma matriz $2\times2$  que corresponde ao tipo específico de aresta, e $p$ igual ao número de regiões retangulares em um caractere têm-se:


 \begin{figure}[!htb]
\centering
\begin{center}
    \includegraphics[scale=0.1, bb= 0 0 2986 1303]{imagens/matriz.PNG}
\end{center}
\caption{Matrizes referentes aos tipos de arestas.}
\label{fig:matriz}
\end{figure}

O vetor de características de saída é ilustrado pelo padrão abaixo. A notação $h_j@r_i$ significa "número de ocorrências de um tipo de aresta representado pela matriz $h_j$ na região $r_i$", \ref{fig:vetor} 


\begin{figure}[!Htb]
\centering
\begin{center}
\includegraphics[scale=0.25
, bb= 0 0 1231 166]{imagens/vetorCaracteristicas.PNG}
\end{center}
\caption{Vetor de Características.}
\label{fig:vetor}
\end{figure}

\end{itemize}


Outra forma de extração de características é a análise estrutural do padrão. Através desse tipo de extração é possível diferenciar padrões por suas características mais substanciais. No caso de reconhecimento de caracteres, a análise estrutural leva em consideração estruturas mais complexas, como junções, terminação de linha e \textit{loops}.

\begin{itemize}
\item Terminação de Linha: é representada por um ponto que possui exatamente um vizinho de pixel preto na 8-vizinhança (Figura \ref{fig:junctions} (c)).   
   
\item Junções: consiste em um ponto que possui pelo menos três pixels pretos na 8-vizinhança. No presente trabalho, consideraram-se apenas dois tipos de junções: com três e quatro vizinhos (Figura \ref{fig:junctions} (a),(b)).

\begin{figure}[!htb]
\centering
\begin{center}
\includegraphics[scale=0.5, bb= 0 0 367 106]{imagens/junctions.PNG}
\end{center}
\caption{(a) Exemplo de junção com quatro vizinhos (b) Exemplo de junção com três vizinhos (c) Exemplo de terminação de linha.}
\label{fig:junctions}
\end{figure}

\item \textit{Loops}: esta é a característica estrutural mais complexa de ser extraída em um caractere. Neste trabalho, o processo de contagem de \textit{loops} trabalha com a imagem negativa do caractere (Figura \ref{fig:loops}), ou seja, o fundo da imagem é representado pela cor preta, enquanto que o caractere é representado pela cor branca. O número de loops pode ser calculado como o número de grupos de pixels pretos na imagem negativa, representado na Figura \ref{fig:loops} pelos números 1, 2, 3, subtraído de um. Essa subtração é feita para desconsiderar o fundo da imagem como um \textit{loop}.

\begin{figure}[!htb]
\centering
\begin{center}
\includegraphics[scale=0.5, bb= 0 0 245 147]{imagens/loops.PNG}
\end{center}
\caption{(a) Imagem original (b) Imagem negativa.}
\label{fig:loops}
\end{figure}
\end{itemize} 

O \textit{toolkit} Horus também fornece algumas implementações de algoritmos para análise estrutural de caracteres.

\subsection{Reconhecimento de Objetos}
	Reconhecimento de objetos é o processo de identificação de um determinado objeto através de suas características. Normalmente, esse processo se inicia com a captura de informações sobre o objeto através de câmeras ou outros tipos de sensores, como sonares por exemplo. Em seguida, essas informações passam pelo processo de extração de características com a finalidade de se extrair um vetor de informações que identifique unicamente o objeto independente das variações que ele possa apresentar. Por fim, esse vetor de características é passado para o processo de reconhecimento, o qual identifica o objeto através de suas características.

	Para tarefas de reconhecimento, o Horus disponibiliza funções para construção e treinamento de redes neurais através da utilização de uma biblioteca denominada FANN (\textit{Fast Artificial Neural Network}). O FANN é uma biblioteca de código aberto implementada em linguagem C que fornece conectores para diversas linguagens de alto nível, dentre elas pode-se citar: Java, C++, Python e Ruby.

	Outra funcionalidade disponibilizada pelo Horus para reconhecimento de objetos é o módulo de OCR. Esse módulo utiliza uma engine OCR \textit{Open Source} chamada de Tesseract. Essa engine está sob a licença Apache e é escrita nas linguagens de programação C e C++. O módulo OCR do Horus pode ser utilizado em aplicações onde haja a necessidade de se reconhecer textos existentes em imagens. 	       

	O módulo OCR é utilizado nas aplicações ANPR e Ariadnes. No ANPR, esse módulo  é utilizado para reconhecer o texto que se encontra nas placas dos automóveis. Já na aplicação Ariadnes, o agente inteligente utiliza esse módulo para reconhecer os textos que se encontram nas placas informativas presentes no ambiente.
	
\section{Mapeamento e Navegação}
\label{sec:slam}
 Chamamos de mapeamento o processo de identificar locais no ambiente do simulador e representá-los em um grafo. O mapeamento no Horus utiliza uma técnica genérica denominada SLAM. Nessa técnica, um agente consegue realizar o mapeamento e a localização no ambiente de forma simultânea. Os dispositivos utilizados pela implementação da técnica SLAM são lasers, para identificar obstáculos, e um odômetro, para medir distâncias percorridas.
 
	O SLAM é composto por vários procedimentos interligados. Cada um desses procedimentos pode ser implementado de diversas formas. Dentre os procedimentos implementados no Horus, podemos citar:

\begin{enumerate}
	\item \textit{Landmark Extraction}: procedimento responsável pela extração de marcos no ambiente.
	\item \textit{Data Association}: procedimento que associa os dados extraídos de um mesmo marco por diferentes leituras de lasers.
	\item \textit{State Estimation}: procedimento responsável por estimar a posição atual do robô com base em seu odômetro e nas extrações de marcos no ambiente.
	\item \textit{State Update}: procedimento que atualiza o estado atual do agente.
	\item \textit{Landmark Update}: procedimento que atualiza as posições dos marcos no ambiente em relação ao agente. 
\end{enumerate}

  Neste trabalho, a proposta utilizada é mapear o ambiente através de um grafo conexo, cujos nós referem-se a: entradas/saídas do ambiente, acessos aos cômodos, obstáculos fixos e esquinas. O peso das arestas é calculado de acordo com a distância euclidiana entre os vértices. 

  O problema de navegação consiste na localização e definição do caminho que o agente deve seguir. Após a construção de uma representação do ambiente em forma de um grafo, o agente é capaz de se localizar e se movimentar pelo ambiente através dos vértices e arestas, previamente mapeados no grafo. Para a utilização de grafos, o Horus fornece classes para sua construção e algoritmos para cálculo de caminho mínimo.

\section{Aplicações desenvolvidas com o Horus}
\label{sec:app}

As aplicações desenvolvidas nesse projeto têm o objetivo de validar e demonstrar a utilidade do \textit{toolkit} Horus no desenvolvimento de aplicações envolvendo agentes inteligentes, visão computacional, mapeamento automático de ambientes e matemática. São três as aplicações desenvolvidas no projeto como um todo, porém, neste trabalho serão detalhadas somente as aplicações que possuem aspectos de visão computacional.

As aplicações desenvolvidas foram: um sistema de reconhecimento automático de placas de automóveis denominado \textit{PyANPR}, descrito no Capítulo \ref{cap:anpr}, um simulador para mapeamento automático de ambientes desconhecidos através da técnica SLAM, chamado \textit{Teseu} e um simulador de movimentação autônoma em ambientes desconhecidos, utilizando visão computacional, denominado \textit{Ariadnes}.

	Teseu é um sistema que simula a movimentação autônoma de um agente inteligente em um ambiente desconhecido. Nele é possível simular o comportamento de um robô real no que tange mapeamento e navegação. O agente tem o objetivo de explorar todo o ambiente utilizando técnicas de localização, mapeamento e exploração implementadas no Horus. 
   A construção do ambiente virtual foi realizada utilizando um software de modelagem 3D \textit{Open Source} chamado Blender 3D. Esse é um produto criado pela \textit{Blender Foundation}, feito na linguagem de programação Python e está sob a licença GNU(\textit{General Public License}). Outra ferramenta utilizada na construção dos simuladores é o Panda3D, criada pela equipe \textit{Walt Disney} para renderização de jogos e ambientes em terceira dimensão. Esse produto também foi criado na linguagem de programação Python e encontra-se sob a licença de uso BSD(\textit{Berkeley Software Distribution }). O uso do Panda3D no Horus restringe-se apenas na manipulação e renderização do ambiente e atores criados previamente no Blender3D. 

O simulador Ariadnes foi construído utilizando as mesmas ferramentas utilizadas na construção do Teseu, porém, com objetivo e técnicas  diferentes. O Ariadnes será mais bem descrito no Capítulo \ref{cap:ariadnes}.
